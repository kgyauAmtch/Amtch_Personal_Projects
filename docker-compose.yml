# version: '3.8'

services:

  # airflow:
  # image:
  # container_name: airflow
  # environment:

  # ports: 
  #   - 5050:2020
  # volumes:

  # network:
  #  - airflow_network








  mysql_db: 
   image: bitnami/mysql:latest
   container_name: mysql_db
   environment:
     MYSQL_ROOT_PASSWORD: gyaurocks99
     MYSQL_DATABASE: flightdb
     MYSQL_USER: flightuser
     MYSQL_PASSWORD: myuserpass
   ports: 
     - 3307:3306
   volumes:
      - mysql_data:/bitnami/mysql/data # Persistent volume for 
      - ./db/mysql_setup.sql:/docker-entrypoint-startdb.d/mysql_setup.sql  # Custom init SQL script
  #  healthcheck:
  #     test: ["CMD-SHELL", "pg_isready -U proj5kafka -d heart_rate_db"]  # Health check to ensure Postgres is ready
  #     interval: 5s  # Check every 5 seconds
  #     timeout: 5s   # Timeout after 5 seconds
  #     retries: 10   # Retry 10 times before marking the service as unhealthy

   networks:
     - airflow_network

  # postgres_db: 
  #  image: postgres:15
  #  container_name: postgres_db

  #  environment:

  #   ports: 
  #    - 4040:4040


  #   volumes:

  #   networks:
  #   - airflow_network

  # data_processing:
  #  image: python:3.10-slim
  #  container_name: data_processing
  #     # Mount  source code folder
  #  working_dir: /opt/spark/work-dir
  #  environment:
  #     KAFKA_BOOTSTRAP_SERVERS: kafka:9092  # Kafka service name from Docker Compose
  #     KAFKA_TOPIC: producer_to_consumer
  #     POSTGRES_HOST: db
  #     POSTGRES_USER: proj5kafka
  #     POSTGRES_PASSWORD: pass_word
  #     POSTGRES_DB: heart_rate_db
  #  command: >
  #     sh -c "pip install kafka-python  &&python kafka_producer.py"  # Install kafka-python and run producer wait for 30 secs before running

  #  depends_on:
  #     - kafka
  #     # - kafka_consumer
  #  networks:
  #     - real_data_network

volumes:
  mysql_data:  # Define the volume for Postgres data storage

networks:
  airflow_network:
    driver: bridge
